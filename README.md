This repository contains a regularly updated paper list on **On-Policy Distillation**.

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) [![License](https://img.shields.io/badge/License-Apache_2.0-green.svg)](./LICENSE) ![GitHub last commit (branch)](https://img.shields.io/github/last-commit/AnhaoZhao-LLMer/On_Policy_Distillation_Paper_List/main?logo=github&color=blue) ![Static Badge](https://img.shields.io/badge/Contributions-welcome-blue.svg?style=flat) 

## Content

- [Content](#content)
- [Keywords Convention](#keywords-convention)
- [Papers](#papers)
  - [Teacher–Student–Based OPD](#teacherstudentbased-opd)
  - [On-Policy Self-Distillation](#on-policy-self-distillation)
  - [Application](#application)



## Keywords Convention

![](https://img.shields.io/badge/OPD-blue) Abbreviation

![](https://img.shields.io/badge/ICML2026-orange) Conference


## Papers

### Teacher–Student–Based OPD
- **On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes**  
  *Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist, Olivier Bachem*. [[pdf](https://arxiv.org/pdf/2306.13649)], 2023.06. ![](https://img.shields.io/badge/ICLR2024-orange)
- **On-Policy Distillation**  
  *Kevin Lu and Thinking Machines Lab*. [[pdf](https://thinkingmachines.ai/blog/on-policy-distillation/)], 2025.10. ![](https://img.shields.io/badge/Blog2025-orange)
- **Qwen3 Technical Report**  
  *Qwen3 Team*. [[pdf](https://arxiv.org/pdf/2505.09388)], 2025.05. ![](https://img.shields.io/badge/Arxiv2025-orange)
- **Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation**  
  *Wenkai Yang, Weijie Liu, Ruobing Xie, Kai Yang, Saiyong Yang, Yankai Lin*. [[pdf](https://arxiv.org/pdf/2602.12125)], 2026.02. ![](https://img.shields.io/badge/Arxiv2026-orange)

### On-Policy Self-Distillation
- **Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models**  
  *Siyan Zhao, Zhihui Xie, Mengchen Liu, Jing Huang, Guan Pang, Feiyu Chen, Aditya Grover*. [[pdf](https://arxiv.org/pdf/2601.18734)], 2026.01. ![](https://img.shields.io/badge/Arxiv2026-orange)
- **Self-Distillation Enables Continual Learning**  
  *Idan Shenfeld, Mehul Damani, Jonas Hübotter, Pulkit Agrawal*. [[pdf](https://arxiv.org/pdf/2601.19897)], 2026.01. ![](https://img.shields.io/badge/Arxiv2026-orange)
- **Reinforcement Learning via Self-Distillation**  
  *Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause*. [[pdf](https://arxiv.org/pdf/2601.20802)], 2026.01. ![](https://img.shields.io/badge/Arxiv2026-orange)
- **Privileged Information Distillation for Language Models**  
  *Emiliano Penaloza, Dheeraj Vattikonda, Nicolas Gontier, Alexandre Lacoste, Laurent Charlin, Massimo Caccia*. [[pdf](https://arxiv.org/pdf/2602.04942)], 2026.02. ![](https://img.shields.io/badge/Arxiv2026-orange)
- **Multi-Token Prediction via Self-Distillation**  
  *John Kirchenbauer, Abhimanyu Hans, Brian Bartoldson, Micah Goldblum, Ashwinee Panda, Tom Goldstein*. [[pdf](https://arxiv.org/pdf/2602.06019)], 2026.02. ![](https://img.shields.io/badge/Arxiv2026-orange)
- **On-Policy Context Distillation for Language Models**  
  *Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei*. [[pdf](https://arxiv.org/pdf/2602.12275)], 2026.02. ![](https://img.shields.io/badge/Arxiv2026-orange)

### Application
- **Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation**  
  *Mengdan Zhu, Yufan Zhao, Tao Di, Yulan Yan, Liang Zhao*. [[pdf](https://arxiv.org/pdf/2602.15005)], 2026.02. ![](https://img.shields.io/badge/Arxiv2026-orange)